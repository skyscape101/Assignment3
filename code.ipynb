{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('2 start restaurants with inside dining', {'entities': [(0, 1, 'Rating'), (2, 7, 'Rating'), (25, 31, 'Amenity'), (32, 38, 'Amenity')]})\n",
      "('34', {'entities': []})\n",
      "('5 star resturants in my town', {'entities': [(0, 1, 'Rating'), (2, 6, 'Rating'), (18, 20, 'Location'), (21, 23, 'Location'), (24, 28, 'Location')]})\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "file_path = 'restauranttrain.bio'\n",
    "\n",
    "with open(file_path, 'r') as f:\n",
    "    content = f.read()\n",
    "\n",
    "# Split content by double newline as each block represents a data point.\n",
    "blocks = content.strip().split('\\n\\n')\n",
    "\n",
    "train_data = []\n",
    "\n",
    "for block in blocks:\n",
    "    lines = block.split('\\n')\n",
    "    sentence = []\n",
    "    entities = []\n",
    "    start_idx = 0\n",
    "    \n",
    "    for line in lines:\n",
    "        if line.startswith('O\\t'):\n",
    "            # \"O\" represents tokens that are not part of any entity.\n",
    "            token = line[2:]\n",
    "            sentence.append(token)\n",
    "            start_idx += len(token) + 1  # 1 is for the space between tokens.\n",
    "        else:\n",
    "            # Extract entity information.\n",
    "            tag, token = re.match(r'(B|I)-(\\w+)\\t(.+)', line).groups()[1:]\n",
    "            sentence.append(token)\n",
    "            end_idx = start_idx + len(token)\n",
    "            entities.append((start_idx, end_idx, tag))\n",
    "            start_idx = end_idx + 1  # 1 is for the space between tokens.\n",
    "\n",
    "    train_data.append((' '.join(sentence), {'entities': entities}))\n",
    "\n",
    "# Print some examples to make sure.\n",
    "for i in range(3):\n",
    "    print(train_data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.training.example import Example\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "ner = nlp.add_pipe('ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, annotations in train_data:\n",
    "    for ent in annotations.get(\"entities\"):\n",
    "        ner.add_label(ent[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  20%|██        | 1/5 [01:25<05:42, 85.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 17384.022361525844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  40%|████      | 2/5 [02:51<04:17, 85.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 12887.014031802471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  60%|██████    | 3/5 [04:16<02:51, 85.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 11462.276149228494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  80%|████████  | 4/5 [05:42<01:25, 85.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 10489.521462399167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 5/5 [07:09<00:00, 85.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 9887.95163881269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "optimizer = nlp.begin_training()\n",
    "\n",
    "for i in tqdm(range(5), desc='Epochs'):  # Number of training iterations\n",
    "    total_loss = 0\n",
    "    for text, annotations in tqdm(train_data, desc='Training', leave=False):\n",
    "        doc = nlp.make_doc(text)\n",
    "        example = Example.from_dict(doc, annotations)\n",
    "        losses = {}\n",
    "        nlp.update([example], drop=0.2, losses=losses)\n",
    "        total_loss += losses['ner']\n",
    "    print(f\"Epoch {i}, Loss: {total_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.to_disk(\"path_to_save_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Rating': 'star', 'Location': 'town'}\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"path_to_save_model\")\n",
    "doc = nlp(\"5 star resturants in my town\")\n",
    "\n",
    "# Extract entities\n",
    "entities = {ent.label_: ent.text for ent in doc.ents}\n",
    "print(entities)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
